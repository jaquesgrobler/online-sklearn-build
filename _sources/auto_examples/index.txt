


.. raw:: html


    <style type="text/css">

    div#sidebarbutton {
        display: none;
    }

    .figure {
        float: left;
        margin: 10px;
        -webkit-border-radius: 10px; /* Saf3-4, iOS 1-3.2, Android <1.6 */
        -moz-border-radius: 10px; /* FF1-3.6 */
        border-radius: 10px; /* Opera 10.5, IE9, Saf5, Chrome, FF4, iOS 4, Android 2.1+ */
        border: 2px solid #fff;
        background-color: white;
        /* --> Thumbnail image size */
        width: 150px;
        height: 100px;
        -webkit-background-size: 150px 100px; /* Saf3-4 */
        -moz-background-size: 150px 100px; /* FF3.6 */
    }

    .figure img {
        display: inline;
    }

    div.docstringWrapper p.caption {
        display: block;
        -webkit-box-shadow: 0px 0px 20px rgba(0, 0, 0, 0.0);
        -moz-box-shadow: 0px 0px 20px rgba(0, 0, 0, .0); /* FF3.5 - 3.6 */
        box-shadow: 0px 0px 20px rgba(0, 0, 0, 0.0); /* Opera 10.5, IE9, FF4+, Chrome 10+ */
        padding: 0px
    }

    div.docstringWrapper p {
        display: none;
        background-color: white;
        -webkit-box-shadow: 0px 0px 20px rgba(0, 0, 0, 1.00);
        -moz-box-shadow: 0px 0px 20px rgba(0, 0, 0, 1.00); /* FF3.5 - 3.6 */
        box-shadow: 0px 0px 20px rgba(0, 0, 0, 1.00); /* Opera 10.5, IE9, FF4+, Chrome 10+ */
        padding: 13px;
        margin-top: 0px;
    }


    </style>


.. raw:: html


        <script type="text/javascript">

        function animateClone(e){
          var position;
          position = $(this).position();
          var clone = $(this).closest('.thumbnailContainer').find('.clonedItem');
          var clone_fig = clone.find('.figure');
          clone.css("left", position.left - (position.left/10)).css("position", "absolute").css("z-index", 1000).css("background-color", "white");
          clone.find('p').show();
          var cloneImg = clone_fig.find('img');

          clone.show();
          clone_fig.show();
          clone.animate({
                height: "270px",
                width: "320px"
            }, 10
          );
          clone_fig.css({
               'margin-top': '20px',
          });
          clone_fig.animate({
               height: "240px",
               width: "305px"
            }, 10
          );
          cloneImg.css({
                'max-height': "200px",
                'max-width': "280px"
            });
          cloneImg.animate({
                height: "200px",
                width: "280px"
            }, 10
          );
          clone.bind("mouseleave", function(e){
              clone.animate({
                  height: "100px",
                  width: "150px"
              }, 10, function(){$(this).hide();});
              clone_fig.animate({
                  height: "100px",
                  width: "150px"
              }, 10, function(){$(this).hide();});
          });
        } //end animateClone()


        $(window).load(function () {
            $(".figure").css("z-index", 1);

            $(".docstringWrapper").each(function(i, obj){
                var clone;
                var $obj = $(obj);
                clone = $obj.clone();
                clone.addClass("clonedItem");
                clone.appendTo($obj.closest(".thumbnailContainer"));
                clone.hide();
                $obj.bind("mouseenter", animateClone);
            }); // end each
        }); // end

        </script>



Examples
========

.. _examples-index:



.. _general_examples:

General examples
----------------

General-purpose and introductory examples for the scikit.





.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/feature_selection_pipeline.png
   :target: ./feature_selection_pipeline.html

   :ref:`example_feature_selection_pipeline.py`


.. raw:: html


    <p>Simple usage of Pipeline that runs successively a univariate feature selection with anova and then a C-SVM of the selected features. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./feature_selection_pipeline



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_rfe_digits.png
   :target: ./plot_rfe_digits.html

   :ref:`example_plot_rfe_digits.py`


.. raw:: html


    <p>A recursive feature elimination example showing the relevance of pixels in a digit classification task.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_rfe_digits



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_rfe_with_cross_validation.png
   :target: ./plot_rfe_with_cross_validation.html

   :ref:`example_plot_rfe_with_cross_validation.py`


.. raw:: html


    <p>A recursive feature elimination example with automatic tuning of the number of features selected with cross-validation. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_rfe_with_cross_validation



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_confusion_matrix.png
   :target: ./plot_confusion_matrix.html

   :ref:`example_plot_confusion_matrix.py`


.. raw:: html


    <p>Example of confusion matrix usage to evaluate the quality of the output of a classifier on the iris data set. The diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier. The higher the diagonal values of the confusion matrix the better, indicating many correct predictions. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_confusion_matrix



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_precision_recall.png
   :target: ./plot_precision_recall.html

   :ref:`example_plot_precision_recall.py`


.. raw:: html


    <p>Example of Precision-Recall metric to evaluate the quality of the output of a classifier. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_precision_recall



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_isotonic_regression.png
   :target: ./plot_isotonic_regression.html

   :ref:`example_plot_isotonic_regression.py`


.. raw:: html


    <p>An illustration of the isotonic regression on generated data. The isotonic regression finds a non-decreasing approximation of a function while minimizing the mean squared error on the training data. The benefit of such a model is that it does not assume any form for the target function such as linearity. For comparison a linear regression is also presented.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_isotonic_regression



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/feature_stacker.png
   :target: ./feature_stacker.html

   :ref:`example_feature_stacker.py`


.. raw:: html


    <p>In many real-world examples, there are many ways to extract features from a dataset. Often it is benefitial to combine several methods to obtain good performance. This example shows how to use ``FeatureUnion`` to combine features obtained by PCA and univariate selection.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./feature_stacker



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_roc.png
   :target: ./plot_roc.html

   :ref:`example_plot_roc.py`


.. raw:: html


    <p>Example of Receiver operating characteristic (ROC) metric to evaluate the quality of the output of a classifier.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_roc



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_hmm_sampling.png
   :target: ./plot_hmm_sampling.html

   :ref:`example_plot_hmm_sampling.py`


.. raw:: html


    <p>This script shows how to sample points from a Hiden Markov Model (HMM): we use a 4-components with specified mean and covariance.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_hmm_sampling



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_digits_classification.png
   :target: ./plot_digits_classification.html

   :ref:`example_plot_digits_classification.py`


.. raw:: html


    <p>An example showing how the scikit-learn can be used to recognize images of hand-written digits.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_digits_classification



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_digits_pipe.png
   :target: ./plot_digits_pipe.html

   :ref:`example_plot_digits_pipe.py`


.. raw:: html


    <p>The PCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_digits_pipe



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_permutation_test_for_classification.png
   :target: ./plot_permutation_test_for_classification.html

   :ref:`example_plot_permutation_test_for_classification.py`


.. raw:: html


    <p>In order to test if a classification score is significative a technique in repeating the classification procedure after randomizing, permuting, the labels. The p-value is then given by the percentage of runs for which the score obtained is greater than the classification score obtained in the first place.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_permutation_test_for_classification



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_roc_crossval.png
   :target: ./plot_roc_crossval.html

   :ref:`example_plot_roc_crossval.py`


.. raw:: html


    <p>Example of Receiver operating characteristic (ROC) metric to evaluate the quality of the output of a classifier using cross-validation. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_roc_crossval



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/grid_search_digits.png
   :target: ./grid_search_digits.html

   :ref:`example_grid_search_digits.py`


.. raw:: html


    <p>The classifier is optimized by "nested" cross-validation using the :class:`sklearn.grid_search.GridSearchCV` object on a development set that comprises only half of the available labeled data.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./grid_search_digits



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_train_error_vs_test_error.png
   :target: ./plot_train_error_vs_test_error.html

   :ref:`example_plot_train_error_vs_test_error.py`


.. raw:: html


    <p>Illustration of how the performance of an estimator on unseen data (test data) is not the same as the performance on training data. As the regularization increases the performance on train decreases while the performance on test is optimal within a range of values of the regularization parameter. The example with an Elastic-Net regression model and the performance is measured using the explained variance a.k.a. R^2.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_train_error_vs_test_error



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_feature_selection.png
   :target: ./plot_feature_selection.html

   :ref:`example_plot_feature_selection.py`


.. raw:: html


    <p>An example showing univariate feature selection.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_feature_selection



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_classification_probability.png
   :target: ./plot_classification_probability.html

   :ref:`example_plot_classification_probability.py`


.. raw:: html


    <p>Plot the classification probability for different classifiers. We use a 3 class dataset, and we classify it with a Support Vector classifier, as well as L1 and L2 penalized logistic regression.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_classification_probability



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_multilabel.png
   :target: ./plot_multilabel.html

   :ref:`example_plot_multilabel.py`


.. raw:: html


    <p>This example simulates a multi-label document classification problem. The dataset is generated randomly based on the following process:
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_multilabel



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/grid_search_text_feature_extraction.png
   :target: ./grid_search_text_feature_extraction.html

   :ref:`example_grid_search_text_feature_extraction.py`


.. raw:: html


    <p>The dataset used in this example is the 20 newsgroups dataset which will be automatically downloaded and then cached and reused for the document classification example.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./grid_search_text_feature_extraction



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/hashing_vs_dict_vectorizer.png
   :target: ./hashing_vs_dict_vectorizer.html

   :ref:`example_hashing_vs_dict_vectorizer.py`


.. raw:: html


    <p>Compares FeatureHasher and DictVectorizer by using both to vectorize text documents.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./hashing_vs_dict_vectorizer



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_johnson_lindenstrauss_bound.png
   :target: ./plot_johnson_lindenstrauss_bound.html

   :ref:`example_plot_johnson_lindenstrauss_bound.py`


.. raw:: html


    <p> The `Johnson-Lindenstrauss lemma`_ states that any high dimensional dataset can be randomly projected into a lower dimensional Euclidean space while controlling the distortion in the pairwise distances.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_johnson_lindenstrauss_bound



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/mlcomp_sparse_document_classification.png
   :target: ./mlcomp_sparse_document_classification.html

   :ref:`example_mlcomp_sparse_document_classification.py`


.. raw:: html


    <p>This is an example showing how the scikit-learn can be used to classify documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features instead of standard numpy arrays.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./mlcomp_sparse_document_classification



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_classifier_comparison.png
   :target: ./plot_classifier_comparison.html

   :ref:`example_plot_classifier_comparison.py`


.. raw:: html


    <p>In particular in high dimensional spaces data can more easily be separated linearly and the simplicity of classifiers such as naive Bayes and linear SVMs might lead to better generalization.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_classifier_comparison



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_pls.png
   :target: ./plot_pls.html

   :ref:`example_plot_pls.py`


.. raw:: html


    <p>Simple usage of various PLS flavor: - PLSCanonical - PLSRegression, with multivariate response, a.k.a. PLS2 - PLSRegression, with univariate response, a.k.a. PLS1 - CCA
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_pls



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/document_clustering.png
   :target: ./document_clustering.html

   :ref:`example_document_clustering.py`


.. raw:: html


    <p>This is an example showing how the scikit-learn can be used to cluster documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features instead of standard numpy arrays.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./document_clustering



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_lda_qda.png
   :target: ./plot_lda_qda.html

   :ref:`example_plot_lda_qda.py`


.. raw:: html


    <p>Plot the confidence ellipsoids of each class and decision boundary 
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_lda_qda



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/plot_kernel_approximation.png
   :target: ./plot_kernel_approximation.html

   :ref:`example_plot_kernel_approximation.py`


.. raw:: html


    <p>.. currentmodule:: sklearn.kernel_approximation
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./plot_kernel_approximation



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ./images/thumb/document_classification_20newsgroups.png
   :target: ./document_classification_20newsgroups.html

   :ref:`example_document_classification_20newsgroups.py`


.. raw:: html


    <p>This is an example showing how scikit-learn can be used to classify documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features and demonstrates various classifiers that can efficiently handle sparse matrices.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ./document_classification_20newsgroups


.. raw:: html

    <div style="clear: both"></div>
    


.. _realworld_examples:

Examples based on real world datasets
-------------------------------------

Applications to real world problems with some medium sized datasets or
interactive user interface.





.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: applications/images/thumb/topics_extraction_with_nmf.png
   :target: ./applications/topics_extraction_with_nmf.html

   :ref:`example_applications_topics_extraction_with_nmf.py`


.. raw:: html


    <p>This is a proof of concept application of Non Negative Matrix Factorization of the term frequency matrix of a corpus of documents so as to extract an additive model of the topic structure of the corpus.
    </p></div>
    </div>


.. toctree::
   :hidden:

   applications/topics_extraction_with_nmf



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: applications/images/thumb/plot_outlier_detection_housing.png
   :target: ./applications/plot_outlier_detection_housing.html

   :ref:`example_applications_plot_outlier_detection_housing.py`


.. raw:: html


    <p>This example illustrates the need for robust covariance estimation on a real data set. It is useful both for outlier detection and for a better understanding of the data structure.
    </p></div>
    </div>


.. toctree::
   :hidden:

   applications/plot_outlier_detection_housing



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: applications/images/thumb/plot_hmm_stock_analysis.png
   :target: ./applications/plot_hmm_stock_analysis.html

   :ref:`example_applications_plot_hmm_stock_analysis.py`


.. raw:: html


    <p>This script shows how to use Gaussian HMM. It uses stock price data, which can be obtained from yahoo finance. For more information on how to get stock prices with matplotlib, please refer to date_demo1.py of matplotlib. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   applications/plot_hmm_stock_analysis



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: applications/images/thumb/plot_tomography_l1_reconstruction.png
   :target: ./applications/plot_tomography_l1_reconstruction.html

   :ref:`example_applications_plot_tomography_l1_reconstruction.py`


.. raw:: html


    <p>This example shows the reconstruction of an image from a set of parallel projections, acquired along different angles. Such a dataset is acquired in **computed tomography** (CT).
    </p></div>
    </div>


.. toctree::
   :hidden:

   applications/plot_tomography_l1_reconstruction



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: applications/images/thumb/face_recognition.png
   :target: ./applications/face_recognition.html

   :ref:`example_applications_face_recognition.py`


.. raw:: html


    <p>The dataset used in this example is a preprocessed excerpt of the "Labeled Faces in the Wild", aka LFW_:
    </p></div>
    </div>


.. toctree::
   :hidden:

   applications/face_recognition



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: applications/images/thumb/plot_species_distribution_modeling.png
   :target: ./applications/plot_species_distribution_modeling.html

   :ref:`example_applications_plot_species_distribution_modeling.py`


.. raw:: html


    <p>Modeling species' geographic distributions is an important problem in conservation biology. In this example we model the geographic distribution of two south american mammals given past observations and 14 environmental variables. Since we have only positive examples (there are no unsuccessful observations), we cast this problem as a density estimation problem and use the `OneClassSVM` provided by the package `sklearn.svm` as our modeling tool. The dataset is provided by Phillips et. al. (2006). If available, the example uses `basemap <http://matplotlib.sourceforge.net/basemap/doc/html/>`_ to plot the coast lines and national boundaries of South America.
    </p></div>
    </div>


.. toctree::
   :hidden:

   applications/plot_species_distribution_modeling



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: applications/images/thumb/plot_stock_market.png
   :target: ./applications/plot_stock_market.html

   :ref:`example_applications_plot_stock_market.py`


.. raw:: html


    <p>.. _stock_market:
    </p></div>
    </div>


.. toctree::
   :hidden:

   applications/plot_stock_market



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: applications/images/thumb/wikipedia_principal_eigenvector.png
   :target: ./applications/wikipedia_principal_eigenvector.html

   :ref:`example_applications_wikipedia_principal_eigenvector.py`


.. raw:: html


    <p>A classical way to assert the relative importance of vertices in a graph is to compute the principal eigenvector of the adjacency matrix so as to assign to each vertex the values of the components of the first eigenvector as a centrality score:
    </p></div>
    </div>


.. toctree::
   :hidden:

   applications/wikipedia_principal_eigenvector



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: applications/images/thumb/svm_gui.png
   :target: ./applications/svm_gui.html

   :ref:`example_applications_svm_gui.py`


.. raw:: html


    <p>A simple graphical frontend for Libsvm mainly intended for didactic purposes. You can create data points by point and click and visualize the decision region induced by different kernels and parameter settings.
    </p></div>
    </div>


.. toctree::
   :hidden:

   applications/svm_gui


.. raw:: html

    <div style="clear: both"></div>
    


.. _cluster_examples:

Clustering
----------

Examples concerning the :mod:`sklearn.cluster` package.





.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: cluster/images/thumb/plot_mean_shift.png
   :target: ./cluster/plot_mean_shift.html

   :ref:`example_cluster_plot_mean_shift.py`


.. raw:: html


    <p>Reference:
    </p></div>
    </div>


.. toctree::
   :hidden:

   cluster/plot_mean_shift



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: cluster/images/thumb/plot_lena_ward_segmentation.png
   :target: ./cluster/plot_lena_ward_segmentation.html

   :ref:`example_cluster_plot_lena_ward_segmentation.py`


.. raw:: html


    <p>Compute the segmentation of a 2D image with Ward hierarchical clustering. The clustering is spatially constrained in order for each segmented region to be in one piece. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   cluster/plot_lena_ward_segmentation



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: cluster/images/thumb/plot_digits_agglomeration.png
   :target: ./cluster/plot_digits_agglomeration.html

   :ref:`example_cluster_plot_digits_agglomeration.py`


.. raw:: html


    <p>These images how similar features are merged together using feature agglomeration. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   cluster/plot_digits_agglomeration



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: cluster/images/thumb/plot_affinity_propagation.png
   :target: ./cluster/plot_affinity_propagation.html

   :ref:`example_cluster_plot_affinity_propagation.py`


.. raw:: html


    <p>Reference: Brendan J. Frey and Delbert Dueck, "Clustering by Passing Messages Between Data Points", Science Feb. 2007
    </p></div>
    </div>


.. toctree::
   :hidden:

   cluster/plot_affinity_propagation



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: cluster/images/thumb/plot_lena_segmentation.png
   :target: ./cluster/plot_lena_segmentation.html

   :ref:`example_cluster_plot_lena_segmentation.py`


.. raw:: html


    <p>This example uses :ref:`spectral_clustering` on a graph created from voxel-to-voxel difference on an image to break this image into multiple partly-homogenous regions.
    </p></div>
    </div>


.. toctree::
   :hidden:

   cluster/plot_lena_segmentation



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: cluster/images/thumb/plot_dbscan.png
   :target: ./cluster/plot_dbscan.html

   :ref:`example_cluster_plot_dbscan.py`


.. raw:: html


    <p>Finds core samples of high density and expands clusters from them.
    </p></div>
    </div>


.. toctree::
   :hidden:

   cluster/plot_dbscan



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: cluster/images/thumb/plot_dict_face_patches.png
   :target: ./cluster/plot_dict_face_patches.html

   :ref:`example_cluster_plot_dict_face_patches.py`


.. raw:: html


    <p>This example uses a large dataset of faces to learn a set of 20 x 20 images patches that constitute faces.
    </p></div>
    </div>


.. toctree::
   :hidden:

   cluster/plot_dict_face_patches



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: cluster/images/thumb/plot_lena_compress.png
   :target: ./cluster/plot_lena_compress.html

   :ref:`example_cluster_plot_lena_compress.py`


.. raw:: html


    <p>
    </p></div>
    </div>


.. toctree::
   :hidden:

   cluster/plot_lena_compress



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: cluster/images/thumb/plot_ward_structured_vs_unstructured.png
   :target: ./cluster/plot_ward_structured_vs_unstructured.html

   :ref:`example_cluster_plot_ward_structured_vs_unstructured.py`


.. raw:: html


    <p>Example builds a swiss roll dataset and runs :ref:`hierarchical_clustering` on their position.
    </p></div>
    </div>


.. toctree::
   :hidden:

   cluster/plot_ward_structured_vs_unstructured



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: cluster/images/thumb/plot_segmentation_toy.png
   :target: ./cluster/plot_segmentation_toy.html

   :ref:`example_cluster_plot_segmentation_toy.py`


.. raw:: html


    <p>In this example, an image with connected circles is generated and :ref:`spectral_clustering` is used to separate the circles.
    </p></div>
    </div>


.. toctree::
   :hidden:

   cluster/plot_segmentation_toy



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: cluster/images/thumb/plot_cluster_iris.png
   :target: ./cluster/plot_cluster_iris.html

   :ref:`example_cluster_plot_cluster_iris.py`


.. raw:: html


    <p>The plots display firstly what a K-means algorithm would yield using three clusters. It is then shown what the effect of a bad initialization is on the classification process: By setting n_init to only 1 (default is 10), the amount of times that the algorithm will be run with different centroid seeds is reduced. The next plot displays what using eight clusters would deliver and finally the ground truth.
    </p></div>
    </div>


.. toctree::
   :hidden:

   cluster/plot_cluster_iris



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: cluster/images/thumb/plot_color_quantization.png
   :target: ./cluster/plot_color_quantization.html

   :ref:`example_cluster_plot_color_quantization.py`


.. raw:: html


    <p>Performs a pixel-wise Vector Quantization (VQ) of an image of the summer palace (China), reducing the number of colors required to show the image from 96,615 unique colors to 64, while preserving the overall appearance quality.
    </p></div>
    </div>


.. toctree::
   :hidden:

   cluster/plot_color_quantization



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: cluster/images/thumb/plot_cluster_comparison.png
   :target: ./cluster/plot_cluster_comparison.html

   :ref:`example_cluster_plot_cluster_comparison.py`


.. raw:: html


    <p>This example aims at showing characteristics of different clustering algorithms on datasets that are "interesting" but still in 2D. The last dataset is an example of a 'null' situation for clustering: the data is homogeneous, and there is no good clustering.
    </p></div>
    </div>


.. toctree::
   :hidden:

   cluster/plot_cluster_comparison



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: cluster/images/thumb/plot_feature_agglomeration_vs_univariate_selection.png
   :target: ./cluster/plot_feature_agglomeration_vs_univariate_selection.html

   :ref:`example_cluster_plot_feature_agglomeration_vs_univariate_selection.py`


.. raw:: html


    <p>This example compares 2 dimensionality reduction strategies:
    </p></div>
    </div>


.. toctree::
   :hidden:

   cluster/plot_feature_agglomeration_vs_univariate_selection



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: cluster/images/thumb/plot_kmeans_stability_low_dim_dense.png
   :target: ./cluster/plot_kmeans_stability_low_dim_dense.html

   :ref:`example_cluster_plot_kmeans_stability_low_dim_dense.py`


.. raw:: html


    <p>Evaluate the ability of k-means initializations strategies to make the algorithm convergence robust as measured by the relative standard deviation of the inertia of the clustering (i.e. the sum of distances to the nearest cluster center).
    </p></div>
    </div>


.. toctree::
   :hidden:

   cluster/plot_kmeans_stability_low_dim_dense



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: cluster/images/thumb/plot_kmeans_digits.png
   :target: ./cluster/plot_kmeans_digits.html

   :ref:`example_cluster_plot_kmeans_digits.py`


.. raw:: html


    <p>In this example with compare the various initialization strategies for K-means in terms of runtime and quality of the results.
    </p></div>
    </div>


.. toctree::
   :hidden:

   cluster/plot_kmeans_digits



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: cluster/images/thumb/plot_adjusted_for_chance_measures.png
   :target: ./cluster/plot_adjusted_for_chance_measures.html

   :ref:`example_cluster_plot_adjusted_for_chance_measures.py`


.. raw:: html


    <p>The following plots demonstrate the impact of the number of clusters and number of samples on various clustering performance evaluation metrics.
    </p></div>
    </div>


.. toctree::
   :hidden:

   cluster/plot_adjusted_for_chance_measures



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: cluster/images/thumb/plot_mini_batch_kmeans.png
   :target: ./cluster/plot_mini_batch_kmeans.html

   :ref:`example_cluster_plot_mini_batch_kmeans.py`


.. raw:: html


    <p>We want to compare the performance of the MiniBatchKMeans and KMeans: the MiniBatchKMeans is faster, but gives slightly different results (see :ref:`mini_batch_kmeans`).
    </p></div>
    </div>


.. toctree::
   :hidden:

   cluster/plot_mini_batch_kmeans


.. raw:: html

    <div style="clear: both"></div>
    


.. _covariance_examples:

Covariance estimation
---------------------

Examples concerning the :mod:`sklearn.covariance` package.





.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: covariance/images/thumb/plot_lw_vs_oas.png
   :target: ./covariance/plot_lw_vs_oas.html

   :ref:`example_covariance_plot_lw_vs_oas.py`


.. raw:: html


    <p>The usual covariance maximum likelihood estimate can be regularized using shrinkage. Ledoit and Wolf proposed a close formula to compute the asymptotical optimal shrinkage parameter (minimizing a MSE criterion), yielding the Ledoit-Wolf covariance estimate.
    </p></div>
    </div>


.. toctree::
   :hidden:

   covariance/plot_lw_vs_oas



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: covariance/images/thumb/plot_outlier_detection.png
   :target: ./covariance/plot_outlier_detection.html

   :ref:`example_covariance_plot_outlier_detection.py`


.. raw:: html


    <p>This example illustrates two ways of performing :ref:`outlier_detection` when the amount of contamination is known:
    </p></div>
    </div>


.. toctree::
   :hidden:

   covariance/plot_outlier_detection



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: covariance/images/thumb/plot_sparse_cov.png
   :target: ./covariance/plot_sparse_cov.html

   :ref:`example_covariance_plot_sparse_cov.py`


.. raw:: html


    <p>Using the GraphLasso estimator to learn a covariance and sparse precision from a small number of samples.
    </p></div>
    </div>


.. toctree::
   :hidden:

   covariance/plot_sparse_cov



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: covariance/images/thumb/plot_covariance_estimation.png
   :target: ./covariance/plot_covariance_estimation.html

   :ref:`example_covariance_plot_covariance_estimation.py`


.. raw:: html


    <p>The usual estimator for covariance is the maximum likelihood estimator, :class:`sklearn.covariance.EmpiricalCovariance`. It is unbiased, i.e. it converges to the true (population) covariance when given many observations. However, it can also be beneficial to regularize it, in order to reduce its variance; this, in turn, introduces some bias. This example illustrates the simple regularization used in :ref:`shrunk_covariance` estimators. In particular, it focuses on how to set the amount of regularization, i.e. how to choose the bias-variance trade-off.
    </p></div>
    </div>


.. toctree::
   :hidden:

   covariance/plot_covariance_estimation



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: covariance/images/thumb/plot_mahalanobis_distances.png
   :target: ./covariance/plot_mahalanobis_distances.html

   :ref:`example_covariance_plot_mahalanobis_distances.py`


.. raw:: html


    <p>For Gaussian distributed data, the distance of an observation :math:`x_i` to the mode of the distribution can be computed using its Mahalanobis distance: :math:`d_{(\mu,\Sigma)}(x_i)^2 = (x_i - \mu)'\Sigma^{-1}(x_i - \mu)` where :math:`\mu` and :math:`\Sigma` are the location and the covariance of the underlying gaussian distribution.
    </p></div>
    </div>


.. toctree::
   :hidden:

   covariance/plot_mahalanobis_distances



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: covariance/images/thumb/plot_robust_vs_empirical_covariance.png
   :target: ./covariance/plot_robust_vs_empirical_covariance.html

   :ref:`example_covariance_plot_robust_vs_empirical_covariance.py`


.. raw:: html


    <p>The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in the data set. In such a case, one would have better to use a robust estimator of covariance to garanty that the estimation is resistant to "errorneous" observations in the data set.
    </p></div>
    </div>


.. toctree::
   :hidden:

   covariance/plot_robust_vs_empirical_covariance


.. raw:: html

    <div style="clear: both"></div>
    


.. _dataset_examples:

Dataset examples
-----------------------

Examples concerning the :mod:`sklearn.datasets` package.





.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: datasets/images/thumb/plot_digits_last_image.png
   :target: ./datasets/plot_digits_last_image.html

   :ref:`example_datasets_plot_digits_last_image.py`


.. raw:: html


    <p>See `here <http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits>`_ for more information about this dataset. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   datasets/plot_digits_last_image



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: datasets/images/thumb/plot_random_dataset.png
   :target: ./datasets/plot_random_dataset.html

   :ref:`example_datasets_plot_random_dataset.py`


.. raw:: html


    <p>Plot several randomly generated 2D classification datasets. This example illustrates the `datasets.make_classification` function.
    </p></div>
    </div>


.. toctree::
   :hidden:

   datasets/plot_random_dataset



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: datasets/images/thumb/plot_iris_dataset.png
   :target: ./datasets/plot_iris_dataset.html

   :ref:`example_datasets_plot_iris_dataset.py`


.. raw:: html


    <p>The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length	and Petal Width.
    </p></div>
    </div>


.. toctree::
   :hidden:

   datasets/plot_iris_dataset


.. raw:: html

    <div style="clear: both"></div>
    


.. _decomposition_examples:

Decomposition
-------------

Examples concerning the :mod:`sklearn.decomposition` package.






.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: decomposition/images/thumb/plot_pca_vs_lda.png
   :target: ./decomposition/plot_pca_vs_lda.html

   :ref:`example_decomposition_plot_pca_vs_lda.py`


.. raw:: html


    <p>The Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour and Virginica) with 4 attributes: sepal length, sepal width, petal length and petal width.
    </p></div>
    </div>


.. toctree::
   :hidden:

   decomposition/plot_pca_vs_lda



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: decomposition/images/thumb/plot_ica_blind_source_separation.png
   :target: ./decomposition/plot_ica_blind_source_separation.html

   :ref:`example_decomposition_plot_ica_blind_source_separation.py`


.. raw:: html


    <p>:ref:`ICA` is used to estimate sources given noisy measurements. Imagine 2 instruments playing simultaneously and 2 microphones recording the mixed signals. ICA is used to recover the sources ie. what is played by each instrument.
    </p></div>
    </div>


.. toctree::
   :hidden:

   decomposition/plot_ica_blind_source_separation



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: decomposition/images/thumb/plot_pca_iris.png
   :target: ./decomposition/plot_pca_iris.html

   :ref:`example_decomposition_plot_pca_iris.py`


.. raw:: html


    <p>
    </p></div>
    </div>


.. toctree::
   :hidden:

   decomposition/plot_pca_iris



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: decomposition/images/thumb/plot_kernel_pca.png
   :target: ./decomposition/plot_kernel_pca.html

   :ref:`example_decomposition_plot_kernel_pca.py`


.. raw:: html


    <p>This example shows that Kernel PCA is able to find a projection of the data that makes data linearly separable. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   decomposition/plot_kernel_pca



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: decomposition/images/thumb/plot_ica_vs_pca.png
   :target: ./decomposition/plot_ica_vs_pca.html

   :ref:`example_decomposition_plot_ica_vs_pca.py`


.. raw:: html


    <p>Illustrate visually the results of :ref:`ICA` vs :ref:`PCA` in the feature space.
    </p></div>
    </div>


.. toctree::
   :hidden:

   decomposition/plot_ica_vs_pca



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: decomposition/images/thumb/plot_sparse_coding.png
   :target: ./decomposition/plot_sparse_coding.html

   :ref:`example_decomposition_plot_sparse_coding.py`


.. raw:: html


    <p>Transform a signal as a sparse combination of Ricker wavelets. This example visually compares different sparse coding methods using the :class:`sklearn.decomposition.SparseCoder` estimator. The Ricker (also known as mexican hat or the second derivative of a gaussian) is not a particularly good kernel to represent piecewise constant signals like this one. It can therefore be seen how much adding different widths of atoms matters and it therefore motivates learning the dictionary to best fit your type of signals.
    </p></div>
    </div>


.. toctree::
   :hidden:

   decomposition/plot_sparse_coding



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: decomposition/images/thumb/plot_pca_3d.png
   :target: ./decomposition/plot_pca_3d.html

   :ref:`example_decomposition_plot_pca_3d.py`


.. raw:: html


    <p>These figures aid in illustrating how a point cloud can be very flat in one direction--which is where PCA comes in to choose a direction that is not flat.
    </p></div>
    </div>


.. toctree::
   :hidden:

   decomposition/plot_pca_3d



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: decomposition/images/thumb/plot_faces_decomposition.png
   :target: ./decomposition/plot_faces_decomposition.html

   :ref:`example_decomposition_plot_faces_decomposition.py`


.. raw:: html


    <p>This example applies to :ref:`olivetti_faces` different unsupervised matrix decomposition (dimension reduction) methods from the module :py:mod:`sklearn.decomposition` (see the documentation chapter :ref:`decompositions`) .
    </p></div>
    </div>


.. toctree::
   :hidden:

   decomposition/plot_faces_decomposition



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: decomposition/images/thumb/plot_image_denoising.png
   :target: ./decomposition/plot_image_denoising.html

   :ref:`example_decomposition_plot_image_denoising.py`


.. raw:: html


    <p>An example comparing the effect of reconstructing noisy fragments of Lena using online :ref:`DictionaryLearning` and various transform methods.
    </p></div>
    </div>


.. toctree::
   :hidden:

   decomposition/plot_image_denoising


.. raw:: html

    <div style="clear: both"></div>
    


.. _ensemble_examples:

Ensemble methods
----------------

Examples concerning the :mod:`sklearn.ensemble` package.





.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ensemble/images/thumb/plot_adaboost_regression.png
   :target: ./ensemble/plot_adaboost_regression.html

   :ref:`example_ensemble_plot_adaboost_regression.py`


.. raw:: html


    <p>A decision tree is boosted using the AdaBoost.R2 [1] algorithm on a 1D sinusoidal dataset with a small amount of Gaussian noise. 299 boosts (300 decision trees) is compared with a single decision tree regressor. As the number of boosts is increased the regressor can fit more detail.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ensemble/plot_adaboost_regression



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ensemble/images/thumb/plot_forest_importances_faces.png
   :target: ./ensemble/plot_forest_importances_faces.html

   :ref:`example_ensemble_plot_forest_importances_faces.py`


.. raw:: html


    <p>This example shows the use of forests of trees to evaluate the importance of the pixels in an image classification task (faces). The hotter the pixel, the more important.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ensemble/plot_forest_importances_faces



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ensemble/images/thumb/plot_forest_importances.png
   :target: ./ensemble/plot_forest_importances.html

   :ref:`example_ensemble_plot_forest_importances.py`


.. raw:: html


    <p>This examples shows the use of forests of trees to evaluate the importance of features on an artificial classification task. The red bars are the feature importances of the forest, along with their inter-trees variability.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ensemble/plot_forest_importances



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ensemble/images/thumb/plot_forest_multioutput.png
   :target: ./ensemble/plot_forest_multioutput.html

   :ref:`example_ensemble_plot_forest_multioutput.py`


.. raw:: html


    <p>This example shows the use of multi-output forests to complete images. The goal is to predict the lower half of a face given its upper half.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ensemble/plot_forest_multioutput



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ensemble/images/thumb/plot_gradient_boosting_regularization.png
   :target: ./ensemble/plot_gradient_boosting_regularization.html

   :ref:`example_ensemble_plot_gradient_boosting_regularization.py`


.. raw:: html


    <p>Illustration of the effect of different regularization strategies for Gradient Boosting. The example is taken from Hastie et al 2009.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ensemble/plot_gradient_boosting_regularization



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ensemble/images/thumb/plot_gradient_boosting_regression.png
   :target: ./ensemble/plot_gradient_boosting_regression.html

   :ref:`example_ensemble_plot_gradient_boosting_regression.py`


.. raw:: html


    <p>Demonstrate Gradient Boosting on the boston housing dataset.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ensemble/plot_gradient_boosting_regression



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ensemble/images/thumb/plot_partial_dependence.png
   :target: ./ensemble/plot_partial_dependence.html

   :ref:`example_ensemble_plot_partial_dependence.py`


.. raw:: html


    <p>Partial dependence plots show the dependence between the target function [1]_ and a set of 'target' features, marginalizing over the values of all other features (the complement features). Due to the limits of human perception the size of the target feature set must be small (usually, one or two) thus the target features are usually chosen among the most important features (see :attr:`~sklearn.ensemble.GradientBoostingRegressor.feature_importances_`).
    </p></div>
    </div>


.. toctree::
   :hidden:

   ensemble/plot_partial_dependence



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ensemble/images/thumb/plot_gradient_boosting_quantile.png
   :target: ./ensemble/plot_gradient_boosting_quantile.html

   :ref:`example_ensemble_plot_gradient_boosting_quantile.py`


.. raw:: html


    <p>This example shows how quantile regression can be used to create prediction intervals. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   ensemble/plot_gradient_boosting_quantile



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ensemble/images/thumb/plot_adaboost_twoclass.png
   :target: ./ensemble/plot_adaboost_twoclass.html

   :ref:`example_ensemble_plot_adaboost_twoclass.py`


.. raw:: html


    <p>This example fits an AdaBoosted decision stump on a non-linearly separable classification dataset composed of two "Gaussian quantiles" clusters (see :func:`sklearn.datasets.make_gaussian_quantiles`) and plots the decision boundary and decision scores. The distributions of decision scores are shown separately for samples of class A and B. The predicted class label for each sample is determined by the sign of the decision score. Samples with decision scores greater than zero are classified as B, and are otherwise classified as A. The magnitude of a decision score determines the degree of likeness with the predicted class label. Additionally, a new dataset could be constructed containing a desired purity of class B, for example, by only selecting samples with a decision score above some value.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ensemble/plot_adaboost_twoclass



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ensemble/images/thumb/plot_random_forest_embedding.png
   :target: ./ensemble/plot_random_forest_embedding.html

   :ref:`example_ensemble_plot_random_forest_embedding.py`


.. raw:: html


    <p>RandomTreesEmbedding provides a way to map data to a very high-dimensional, sparse representation, which might be beneficial for classification. The mapping is completely unsupervised and very efficient.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ensemble/plot_random_forest_embedding



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ensemble/images/thumb/plot_forest_iris.png
   :target: ./ensemble/plot_forest_iris.html

   :ref:`example_ensemble_plot_forest_iris.py`


.. raw:: html


    <p>Plot the decision surfaces of forests of randomized trees trained on pairs of features of the iris dataset.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ensemble/plot_forest_iris



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ensemble/images/thumb/plot_adaboost_multiclass.png
   :target: ./ensemble/plot_adaboost_multiclass.html

   :ref:`example_ensemble_plot_adaboost_multiclass.py`


.. raw:: html


    <p>This example reproduces Figure 1 of Zhu et al [1] and shows how boosting can improve prediction accuracy on a multi-class problem. The classification dataset is constructed by taking a ten-dimensional standard normal distribution and defining three classes separated by nested concentric ten-dimensional spheres such that roughly equal numbers of samples are in each class (quantiles of the :math:`\chi^2` distribution).
    </p></div>
    </div>


.. toctree::
   :hidden:

   ensemble/plot_adaboost_multiclass



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: ensemble/images/thumb/plot_adaboost_hastie_10_2.png
   :target: ./ensemble/plot_adaboost_hastie_10_2.html

   :ref:`example_ensemble_plot_adaboost_hastie_10_2.py`


.. raw:: html


    <p>This example is based on Figure 10.2 from Hastie et al 2009 [1] and illustrates the difference in performance between the discrete SAMME [2] boosting algorithm and real SAMME.R boosting algorithm. Both algorithms are evaluated on a binary classification task where the target Y is a non-linear function of 10 input features.
    </p></div>
    </div>


.. toctree::
   :hidden:

   ensemble/plot_adaboost_hastie_10_2


.. raw:: html

    <div style="clear: both"></div>
    


Tutorial exercises
------------------

Exercises for the tutorials





.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: exercises/images/thumb/plot_digits_classification_exercise.png
   :target: ./exercises/plot_digits_classification_exercise.html

   :ref:`example_exercises_plot_digits_classification_exercise.py`


.. raw:: html


    <p>This exercise is used in the :ref:`clf_tut` part of the :ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   exercises/plot_digits_classification_exercise



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: exercises/images/thumb/plot_cv_digits.png
   :target: ./exercises/plot_cv_digits.html

   :ref:`example_exercises_plot_cv_digits.py`


.. raw:: html


    <p>This exercise is used in the :ref:`cv_generators_tut` part of the :ref:`model_selection_tut` section of the :ref:`stat_learn_tut_index`. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   exercises/plot_cv_digits



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: exercises/images/thumb/plot_iris_exercise.png
   :target: ./exercises/plot_iris_exercise.html

   :ref:`example_exercises_plot_iris_exercise.py`


.. raw:: html


    <p>This exercise is used in the :ref:`using_kernels_tut` part of the :ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   exercises/plot_iris_exercise



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: exercises/images/thumb/plot_cv_diabetes.png
   :target: ./exercises/plot_cv_diabetes.html

   :ref:`example_exercises_plot_cv_diabetes.py`


.. raw:: html


    <p>This exercise is used in the :ref:`cv_estimators_tut` part of the :ref:`model_selection_tut` section of the :ref:`stat_learn_tut_index`. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   exercises/plot_cv_diabetes


.. raw:: html

    <div style="clear: both"></div>
    


.. _gaussian_process_examples:

Gaussian Process for Machine Learning
-------------------------------------

Examples concerning the :mod:`sklearn.gaussian_process` package.






.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: gaussian_process/images/thumb/gp_diabetes_dataset.png
   :target: ./gaussian_process/gp_diabetes_dataset.html

   :ref:`example_gaussian_process_gp_diabetes_dataset.py`


.. raw:: html


    <p>This example consists in fitting a Gaussian Process model onto the diabetes dataset.
    </p></div>
    </div>


.. toctree::
   :hidden:

   gaussian_process/gp_diabetes_dataset



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: gaussian_process/images/thumb/plot_gp_probabilistic_classification_after_regression.png
   :target: ./gaussian_process/plot_gp_probabilistic_classification_after_regression.html

   :ref:`example_gaussian_process_plot_gp_probabilistic_classification_after_regression.py`


.. raw:: html


    <p>A two-dimensional regression exercise with a post-processing allowing for probabilistic classification thanks to the Gaussian property of the prediction.
    </p></div>
    </div>


.. toctree::
   :hidden:

   gaussian_process/plot_gp_probabilistic_classification_after_regression



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: gaussian_process/images/thumb/plot_gp_regression.png
   :target: ./gaussian_process/plot_gp_regression.html

   :ref:`example_gaussian_process_plot_gp_regression.py`


.. raw:: html


    <p>A simple one-dimensional regression exercise computed in two different ways:
    </p></div>
    </div>


.. toctree::
   :hidden:

   gaussian_process/plot_gp_regression


.. raw:: html

    <div style="clear: both"></div>
    


.. _linear_examples:

Generalized Linear Models
-------------------------

Examples concerning the :mod:`sklearn.linear_model` package.





.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_lasso_lars.png
   :target: ./linear_model/plot_lasso_lars.html

   :ref:`example_linear_model_plot_lasso_lars.py`


.. raw:: html


    <p>Computes Lasso Path along the regularization parameter using the LARS algorithm on the diabetest dataset. Each color represents a different feature of the coefficient vector, and this is displayed as a function of the regularization parameter.
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_lasso_lars



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_sgd_separating_hyperplane.png
   :target: ./linear_model/plot_sgd_separating_hyperplane.html

   :ref:`example_linear_model_plot_sgd_separating_hyperplane.py`


.. raw:: html


    <p>Plot the maximum margin separating hyperplane within a two-class separable dataset using a linear Support Vector Machines classifier trained using SGD. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_sgd_separating_hyperplane



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_polynomial_interpolation.png
   :target: ./linear_model/plot_polynomial_interpolation.html

   :ref:`example_linear_model_plot_polynomial_interpolation.py`


.. raw:: html


    <p>This example demonstrates how to approximate a function with a polynomial of degree n_degree by using ridge regression. Concretely, from n_samples 1d points, it suffices to build the Vandermonde matrix, which is n_samples x n_degree+1 and has the following form:
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_polynomial_interpolation



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_sgd_weighted_samples.png
   :target: ./linear_model/plot_sgd_weighted_samples.html

   :ref:`example_linear_model_plot_sgd_weighted_samples.py`


.. raw:: html


    <p>Plot decision function of a weighted dataset, where the size of points is proportional to its weight. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_sgd_weighted_samples



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_ridge_path.png
   :target: ./linear_model/plot_ridge_path.html

   :ref:`example_linear_model_plot_ridge_path.py`


.. raw:: html


    <p>.. currentmodule:: sklearn.linear_model
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_ridge_path



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_sgd_loss_functions.png
   :target: ./linear_model/plot_sgd_loss_functions.html

   :ref:`example_linear_model_plot_sgd_loss_functions.py`


.. raw:: html


    <p>Plot the convex loss functions supported by `sklearn.linear_model.stochastic_gradient`. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_sgd_loss_functions



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_sgd_comparison.png
   :target: ./linear_model/plot_sgd_comparison.html

   :ref:`example_linear_model_plot_sgd_comparison.py`


.. raw:: html


    <p>An example showing how different online solvers perform on the hand-written digits dataset.
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_sgd_comparison



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_logistic_path.png
   :target: ./linear_model/plot_logistic_path.html

   :ref:`example_linear_model_plot_logistic_path.py`


.. raw:: html


    <p>Computes path on IRIS dataset.
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_logistic_path



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_iris_logistic.png
   :target: ./linear_model/plot_iris_logistic.html

   :ref:`example_linear_model_plot_iris_logistic.py`


.. raw:: html


    <p>
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_iris_logistic



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_sgd_weighted_classes.png
   :target: ./linear_model/plot_sgd_weighted_classes.html

   :ref:`example_linear_model_plot_sgd_weighted_classes.py`


.. raw:: html


    <p>Fit linear SVMs with and without class weighting. Allows to handle problems with unbalanced classes.
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_sgd_weighted_classes



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_ols_ridge_variance.png
   :target: ./linear_model/plot_ols_ridge_variance.html

   :ref:`example_linear_model_plot_ols_ridge_variance.py`


.. raw:: html


    <p>Ridge regression is basically minimizing a penalised version of the least-squared function. The penalising `shrinks` the value of the regression coefficients. Despite the few data points in each dimension, the slope of the prediction is much more stable and the variance in the line itself is greatly reduced, in comparison to that of the standard linear regression 
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_ols_ridge_variance



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_ols.png
   :target: ./linear_model/plot_ols.html

   :ref:`example_linear_model_plot_ols.py`


.. raw:: html


    <p>The coefficients, the residual sum of squares and the variance score are also calculated.
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_ols



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_multi_task_lasso_support.png
   :target: ./linear_model/plot_multi_task_lasso_support.html

   :ref:`example_linear_model_plot_multi_task_lasso_support.py`


.. raw:: html


    <p>The multi-task lasso allows to fit multiple regression problems jointly enforcing the selected features to be the same across tasks. This example simulates sequential measurements, each task is a time instant, and the relevant features vary in amplitude over time while being the same. The multi-task lasso imposes that features that are selected at one time point are select for all time point. This makes feature selection by the Lasso more stable.
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_multi_task_lasso_support



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_logistic.png
   :target: ./linear_model/plot_logistic.html

   :ref:`example_linear_model_plot_logistic.py`


.. raw:: html


    <p>
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_logistic



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_sgd_penalties.png
   :target: ./linear_model/plot_sgd_penalties.html

   :ref:`example_linear_model_plot_sgd_penalties.py`


.. raw:: html


    <p>Plot the contours of the three penalties supported by `sklearn.linear_model.stochastic_gradient`.
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_sgd_penalties



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/lasso_dense_vs_sparse_data.png
   :target: ./linear_model/lasso_dense_vs_sparse_data.html

   :ref:`example_linear_model_lasso_dense_vs_sparse_data.py`


.. raw:: html


    <p>We show that linear_model.Lasso provides the same results for dense and sparse data and that in the case of sparse data the speed is improved.
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/lasso_dense_vs_sparse_data



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_omp.png
   :target: ./linear_model/plot_omp.html

   :ref:`example_linear_model_plot_omp.py`


.. raw:: html


    <p>Using orthogonal matching pursuit for recovering a sparse signal from a noisy measurement encoded with a dictionary 
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_omp



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_lasso_and_elasticnet.png
   :target: ./linear_model/plot_lasso_and_elasticnet.html

   :ref:`example_linear_model_plot_lasso_and_elasticnet.py`


.. raw:: html


    <p>Estimates Lasso and Elastic-Net regression models on a manually generated sparse signal corrupted with an additive noise. Estimated coefficients are compared with the ground-truth.
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_lasso_and_elasticnet



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_bayesian_ridge.png
   :target: ./linear_model/plot_bayesian_ridge.html

   :ref:`example_linear_model_plot_bayesian_ridge.py`


.. raw:: html


    <p>Computes a :ref:`bayesian_ridge_regression` on a synthetic dataset.
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_bayesian_ridge



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_ols_3d.png
   :target: ./linear_model/plot_ols_3d.html

   :ref:`example_linear_model_plot_ols_3d.py`


.. raw:: html


    <p>Features 1 and 2 of the diabetes-dataset are fitted and plotted below. It illustrates that although feature 2 has a strong coefficient on the full model, it does not give us much regarding `y` when compared to just feautre 1
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_ols_3d



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_ard.png
   :target: ./linear_model/plot_ard.html

   :ref:`example_linear_model_plot_ard.py`


.. raw:: html


    <p>Fit regression model with :ref:`bayesian_ridge_regression`.
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_ard



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_logistic_l1_l2_sparsity.png
   :target: ./linear_model/plot_logistic_l1_l2_sparsity.html

   :ref:`example_linear_model_plot_logistic_l1_l2_sparsity.py`


.. raw:: html


    <p>Comparison of the sparsity (percentage of zero coefficients) of solutions when L1 and L2 penalty are used for different values of C. We can see that large values of C give more freedom to the model.  Conversely, smaller values of C constrain the model more. In the L1 penalty case, this leads to sparser solutions.
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_logistic_l1_l2_sparsity



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_sgd_iris.png
   :target: ./linear_model/plot_sgd_iris.html

   :ref:`example_linear_model_plot_sgd_iris.py`


.. raw:: html


    <p>Plot decision surface of multi-class SGD on iris dataset. The hyperplanes corresponding to the three one-versus-all (OVA) classifiers are represented by the dashed lines.
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_sgd_iris



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_lasso_coordinate_descent_path.png
   :target: ./linear_model/plot_lasso_coordinate_descent_path.html

   :ref:`example_linear_model_plot_lasso_coordinate_descent_path.py`


.. raw:: html


    <p>Lasso and elastic net (L1 and L2 penalisation) implemented using a coordinate descent.
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_lasso_coordinate_descent_path



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_lasso_model_selection.png
   :target: ./linear_model/plot_lasso_model_selection.html

   :ref:`example_linear_model_plot_lasso_model_selection.py`


.. raw:: html


    <p>Use the Akaike information criterion (AIC), the Bayes Information criterion (BIC) and cross-validation to select an optimal value of the regularization parameter alpha of the :ref:`lasso` estimator.
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_lasso_model_selection



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: linear_model/images/thumb/plot_sparse_recovery.png
   :target: ./linear_model/plot_sparse_recovery.html

   :ref:`example_linear_model_plot_sparse_recovery.py`


.. raw:: html


    <p>Given a small number of observations, we want to recover which features of X are relevant to explain y. For this :ref:`sparse linear models <l1_feature_selection>` can outperform standard statistical tests if the true model is sparse, i.e. if a small fraction of the features are relevant.
    </p></div>
    </div>


.. toctree::
   :hidden:

   linear_model/plot_sparse_recovery


.. raw:: html

    <div style="clear: both"></div>
    


.. _manifold_examples:

Manifold learning
-----------------------

Examples concerning the :mod:`sklearn.manifold` package.






.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: manifold/images/thumb/plot_swissroll.png
   :target: ./manifold/plot_swissroll.html

   :ref:`example_manifold_plot_swissroll.py`


.. raw:: html


    <p>An illustration of Swiss Roll reduction with locally linear embedding 
    </p></div>
    </div>


.. toctree::
   :hidden:

   manifold/plot_swissroll



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: manifold/images/thumb/plot_mds.png
   :target: ./manifold/plot_mds.html

   :ref:`example_manifold_plot_mds.py`


.. raw:: html


    <p>An illustration of the metric and non-metric MDS on generated noisy data.
    </p></div>
    </div>


.. toctree::
   :hidden:

   manifold/plot_mds



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: manifold/images/thumb/plot_compare_methods.png
   :target: ./manifold/plot_compare_methods.html

   :ref:`example_manifold_plot_compare_methods.py`


.. raw:: html


    <p>An illustration of dimensionality reduction on the S-curve dataset with various manifold learning methods.
    </p></div>
    </div>


.. toctree::
   :hidden:

   manifold/plot_compare_methods



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: manifold/images/thumb/plot_manifold_sphere.png
   :target: ./manifold/plot_manifold_sphere.html

   :ref:`example_manifold_plot_manifold_sphere.py`


.. raw:: html


    <p>An application of the different :ref:`manifold` techniques on a spherical data-set. Here one can see the use of dimensionality reduction in order to gain some intuition regarding the Manifold learning methods. Regarding the dataset, the poles are cut from the sphere, as well as a thin slice down its side. This enables the manifold learning techniques to 'spread it open' whilst projecting it onto two dimensions.
    </p></div>
    </div>


.. toctree::
   :hidden:

   manifold/plot_manifold_sphere



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: manifold/images/thumb/plot_lle_digits.png
   :target: ./manifold/plot_lle_digits.html

   :ref:`example_manifold_plot_lle_digits.py`


.. raw:: html


    <p>An illustration of various embeddings on the digits dataset.
    </p></div>
    </div>


.. toctree::
   :hidden:

   manifold/plot_lle_digits


.. raw:: html

    <div style="clear: both"></div>
    


.. _mixture_examples:

Gaussian Mixture Models
-----------------------

Examples concerning the :mod:`sklearn.mixture` package.





.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: mixture/images/thumb/plot_gmm_pdf.png
   :target: ./mixture/plot_gmm_pdf.html

   :ref:`example_mixture_plot_gmm_pdf.py`


.. raw:: html


    <p>Plot the density estimation of a mixture of two gaussians. Data is generated from two gaussians with different centers and covariance matrices. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   mixture/plot_gmm_pdf



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: mixture/images/thumb/plot_gmm.png
   :target: ./mixture/plot_gmm.html

   :ref:`example_mixture_plot_gmm.py`


.. raw:: html


    <p>Plot the confidence ellipsoids of a mixture of two gaussians with EM and variational dirichlet process.
    </p></div>
    </div>


.. toctree::
   :hidden:

   mixture/plot_gmm



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: mixture/images/thumb/plot_gmm_sin.png
   :target: ./mixture/plot_gmm_sin.html

   :ref:`example_mixture_plot_gmm_sin.py`


.. raw:: html


    <p>This example highlights the advantages of the Dirichlet Process: complexity control and dealing with sparse data. The dataset is formed by 100 points loosely spaced following a noisy sine curve. The fit by the GMM class, using the expectation-maximization algorithm to fit a mixture of 10 gaussian components, finds too-small components and very little structure. The fits by the dirichlet process, however, show that the model can either learn a global structure for the data (small alpha) or easily interpolate to finding relevant local structure (large alpha), never falling into the problems shown by the GMM class. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   mixture/plot_gmm_sin



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: mixture/images/thumb/plot_gmm_selection.png
   :target: ./mixture/plot_gmm_selection.html

   :ref:`example_mixture_plot_gmm_selection.py`


.. raw:: html


    <p>This example shows that model selection can be perfomed with Gaussian Mixture Models using information-theoretic criteria (BIC). Model selection concerns both the covariance type and the number of components in the model. In that case, AIC also provides the right result (not shown to save time), but BIC is better suited if the problem is to identify the right model. Unlike Bayesian procedures, such inferences are prior-free.
    </p></div>
    </div>


.. toctree::
   :hidden:

   mixture/plot_gmm_selection



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: mixture/images/thumb/plot_gmm_classifier.png
   :target: ./mixture/plot_gmm_classifier.html

   :ref:`example_mixture_plot_gmm_classifier.py`


.. raw:: html


    <p>Demonstration of :ref:`gmm` for classification.
    </p></div>
    </div>


.. toctree::
   :hidden:

   mixture/plot_gmm_classifier


.. raw:: html

    <div style="clear: both"></div>
    


.. _neighbors_examples:

Nearest Neighbors
-----------------------

Examples concerning the :mod:`sklearn.neighbors` package.





.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: neighbors/images/thumb/plot_regression.png
   :target: ./neighbors/plot_regression.html

   :ref:`example_neighbors_plot_regression.py`


.. raw:: html


    <p>Demonstrate the resolution of a regression problem using a k-Nearest Neighbor and the interpolation of the target using both barycenter and constant weights.
    </p></div>
    </div>


.. toctree::
   :hidden:

   neighbors/plot_regression



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: neighbors/images/thumb/plot_classification.png
   :target: ./neighbors/plot_classification.html

   :ref:`example_neighbors_plot_classification.py`


.. raw:: html


    <p>Sample usage of Nearest Neighbors classification. It will plot the decision boundaries for each class. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   neighbors/plot_classification



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: neighbors/images/thumb/plot_nearest_centroid.png
   :target: ./neighbors/plot_nearest_centroid.html

   :ref:`example_neighbors_plot_nearest_centroid.py`


.. raw:: html


    <p>Sample usage of Nearest Centroid classification. It will plot the decision boundaries for each class. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   neighbors/plot_nearest_centroid


.. raw:: html

    <div style="clear: both"></div>
    


.. _semi_supervised_examples:

Semi Supervised Classification
------------------------------

Examples concerning the :mod:`sklearn.semi_supervised` package.





.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: semi_supervised/images/thumb/plot_label_propagation_structure.png
   :target: ./semi_supervised/plot_label_propagation_structure.html

   :ref:`example_semi_supervised_plot_label_propagation_structure.py`


.. raw:: html


    <p>Example of LabelPropagation learning a complex internal structure to demonstrate "manifold learning". The outer circle should be labeled "red" and the inner circle "blue". Because both label groups lie inside their own distinct shape, we can see that the labels propagate correctly around the circle. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   semi_supervised/plot_label_propagation_structure



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: semi_supervised/images/thumb/plot_label_propagation_versus_svm_iris.png
   :target: ./semi_supervised/plot_label_propagation_versus_svm_iris.html

   :ref:`example_semi_supervised_plot_label_propagation_versus_svm_iris.py`


.. raw:: html


    <p>Comparison for decision boundary generated on iris dataset between Label Propagation and SVM.
    </p></div>
    </div>


.. toctree::
   :hidden:

   semi_supervised/plot_label_propagation_versus_svm_iris



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: semi_supervised/images/thumb/plot_label_propagation_digits.png
   :target: ./semi_supervised/plot_label_propagation_digits.html

   :ref:`example_semi_supervised_plot_label_propagation_digits.py`


.. raw:: html


    <p>This example demonstrates the power of semisupervised learning by training a Label Spreading model to classify handwritten digits with sets of very few labels.
    </p></div>
    </div>


.. toctree::
   :hidden:

   semi_supervised/plot_label_propagation_digits



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: semi_supervised/images/thumb/plot_label_propagation_digits_active_learning.png
   :target: ./semi_supervised/plot_label_propagation_digits_active_learning.html

   :ref:`example_semi_supervised_plot_label_propagation_digits_active_learning.py`


.. raw:: html


    <p>Demonstrates an active learning technique to learn handwritten digits using label propagation.
    </p></div>
    </div>


.. toctree::
   :hidden:

   semi_supervised/plot_label_propagation_digits_active_learning


.. raw:: html

    <div style="clear: both"></div>
    


.. _svm_examples:

Support Vector Machines
-----------------------

Examples concerning the :mod:`sklearn.svm` package.





.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: svm/images/thumb/plot_weighted_samples.png
   :target: ./svm/plot_weighted_samples.html

   :ref:`example_svm_plot_weighted_samples.py`


.. raw:: html


    <p>Plot decision function of a weighted dataset, where the size of points is proportional to its weight. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   svm/plot_weighted_samples



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: svm/images/thumb/plot_svm_nonlinear.png
   :target: ./svm/plot_svm_nonlinear.html

   :ref:`example_svm_plot_svm_nonlinear.py`


.. raw:: html


    <p>Perform binary classification using non-linear SVC with RBF kernel. The target to predict is a XOR of the inputs.
    </p></div>
    </div>


.. toctree::
   :hidden:

   svm/plot_svm_nonlinear



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: svm/images/thumb/plot_svm_regression.png
   :target: ./svm/plot_svm_regression.html

   :ref:`example_svm_plot_svm_regression.py`


.. raw:: html


    <p>Toy example of 1D regression using linear, polynominial and RBF kernels.
    </p></div>
    </div>


.. toctree::
   :hidden:

   svm/plot_svm_regression



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: svm/images/thumb/plot_separating_hyperplane.png
   :target: ./svm/plot_separating_hyperplane.html

   :ref:`example_svm_plot_separating_hyperplane.py`


.. raw:: html


    <p>Plot the maximum margin separating hyperplane within a two-class separable dataset using a Support Vector Machines classifier with linear kernel. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   svm/plot_separating_hyperplane



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: svm/images/thumb/plot_separating_hyperplane_unbalanced.png
   :target: ./svm/plot_separating_hyperplane_unbalanced.html

   :ref:`example_svm_plot_separating_hyperplane_unbalanced.py`


.. raw:: html


    <p>Find the optimal separating hyperplane using an SVC for classes that are unbalanced.
    </p></div>
    </div>


.. toctree::
   :hidden:

   svm/plot_separating_hyperplane_unbalanced



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: svm/images/thumb/plot_svm_iris.png
   :target: ./svm/plot_svm_iris.html

   :ref:`example_svm_plot_svm_iris.py`


.. raw:: html


    <p>The decision boundaries, are shown with all the points in the training-set.
    </p></div>
    </div>


.. toctree::
   :hidden:

   svm/plot_svm_iris



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: svm/images/thumb/plot_custom_kernel.png
   :target: ./svm/plot_custom_kernel.html

   :ref:`example_svm_plot_custom_kernel.py`


.. raw:: html


    <p>Simple usage of Support Vector Machines to classify a sample. It will plot the decision surface and the support vectors.
    </p></div>
    </div>


.. toctree::
   :hidden:

   svm/plot_custom_kernel



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: svm/images/thumb/plot_svm_anova.png
   :target: ./svm/plot_svm_anova.html

   :ref:`example_svm_plot_svm_anova.py`


.. raw:: html


    <p>This example shows how to perform univariate feature before running a SVC (support vector classifier) to improve the classification scores. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   svm/plot_svm_anova



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: svm/images/thumb/plot_iris.png
   :target: ./svm/plot_iris.html

   :ref:`example_svm_plot_iris.py`


.. raw:: html


    <p>Comparison of different linear SVM classifiers on the iris dataset. It will plot the decision surface for four different SVM classifiers.
    </p></div>
    </div>


.. toctree::
   :hidden:

   svm/plot_iris



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: svm/images/thumb/plot_oneclass.png
   :target: ./svm/plot_oneclass.html

   :ref:`example_svm_plot_oneclass.py`


.. raw:: html


    <p>:ref:`One-class SVM <svm_outlier_detection>` is an unsupervised algorithm that learns a decision function for novelty detection: classifying new data as similar or different to the training set. 
    </p></div>
    </div>


.. toctree::
   :hidden:

   svm/plot_oneclass



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: svm/images/thumb/plot_svm_kernels.png
   :target: ./svm/plot_svm_kernels.html

   :ref:`example_svm_plot_svm_kernels.py`


.. raw:: html


    <p> 
    </p></div>
    </div>


.. toctree::
   :hidden:

   svm/plot_svm_kernels



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: svm/images/thumb/plot_svm_margin.png
   :target: ./svm/plot_svm_margin.html

   :ref:`example_svm_plot_svm_margin.py`


.. raw:: html


    <p>A small value of `C` includes more/all the observations, allowing the margins to be calculated using all the data in the area.
    </p></div>
    </div>


.. toctree::
   :hidden:

   svm/plot_svm_margin



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: svm/images/thumb/plot_svm_scale_c.png
   :target: ./svm/plot_svm_scale_c.html

   :ref:`example_svm_plot_svm_scale_c.py`


.. raw:: html


    <p>The following example illustrates the effect of scaling the regularization parameter when using :ref:`svm` for :ref:`classification <svm_classification>`. For SVC classification, we are interested in a risk minimization for the equation:
    </p></div>
    </div>


.. toctree::
   :hidden:

   svm/plot_svm_scale_c



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: svm/images/thumb/plot_rbf_parameters.png
   :target: ./svm/plot_rbf_parameters.html

   :ref:`example_svm_plot_rbf_parameters.py`


.. raw:: html


    <p>This example illustrates the effect of the parameters `gamma` and `C` of the rbf kernel SVM.
    </p></div>
    </div>


.. toctree::
   :hidden:

   svm/plot_rbf_parameters


.. raw:: html

    <div style="clear: both"></div>
    


.. _tree_examples:

Decision Trees
--------------

Examples concerning the :mod:`sklearn.tree` package.





.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: tree/images/thumb/plot_tree_regression.png
   :target: ./tree/plot_tree_regression.html

   :ref:`example_tree_plot_tree_regression.py`


.. raw:: html


    <p>1D regression with :ref:`decision trees <tree>`: the decision tree is used to fit a sine curve with addition noisy observation. As a result, it learns local linear regressions approximating the sine curve.
    </p></div>
    </div>


.. toctree::
   :hidden:

   tree/plot_tree_regression



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: tree/images/thumb/plot_tree_regression_multioutput.png
   :target: ./tree/plot_tree_regression_multioutput.html

   :ref:`example_tree_plot_tree_regression_multioutput.py`


.. raw:: html


    <p>Multi-output regression with :ref:`decision trees <tree>`: the decision tree is used to predict simultaneously the noisy x and y observations of a circle given a single underlying feature. As a result, it learns local linear regressions approximating the circle.
    </p></div>
    </div>


.. toctree::
   :hidden:

   tree/plot_tree_regression_multioutput



.. raw:: html


    <div class="thumbnailContainer">
        <div class="docstringWrapper">


.. figure:: tree/images/thumb/plot_iris.png
   :target: ./tree/plot_iris.html

   :ref:`example_tree_plot_iris.py`


.. raw:: html


    <p>Plot the decision surface of a :ref:`decision tree <tree>` trained on pairs of features of the iris dataset.
    </p></div>
    </div>


.. toctree::
   :hidden:

   tree/plot_iris


.. raw:: html

    <div style="clear: both"></div>
    